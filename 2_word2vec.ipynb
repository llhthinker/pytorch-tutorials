{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified ./data/text8.zip\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = f.read(f.namelist()[0]).decode().split()\n",
    "  return data\n",
    "\n",
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "filename = maybe_download('./data/text8.zip', 31344016)\n",
    "words = read_data(filename)\n",
    "print(words[:10])\n",
    "print('Data size %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253854\n",
      "[('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764), ('in', 372201), ('a', 325873), ('to', 316376), ('zero', 264975), ('nine', 250430), ('two', 192644)]\n",
      "442\n"
     ]
    }
   ],
   "source": [
    "vocabs = set(words)\n",
    "vocab_size = len(vocabs)\n",
    "print(vocab_size)\n",
    "print(Counter(words).most_common(10))\n",
    "print(len(set(words[:1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17005207\n",
      "[5235, 3082, 13, 7, 196, 3, 3135, 47, 60, 157]\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(words, vocab_size):\n",
    "    counter = [('UNK', -1)]\n",
    "    counter.extend(Counter(words).most_common(vocab_size-1))\n",
    "    \n",
    "    word2index = dict()\n",
    "    index = 1\n",
    "    for word, _ in counter:\n",
    "        word2index[word] = index\n",
    "        index += 1\n",
    "    \n",
    "    data = list()\n",
    "    for w in words:\n",
    "        if w in word2index:\n",
    "            index = word2index[w]\n",
    "        else:\n",
    "            index = 0\n",
    "        data.append(index)\n",
    "    \n",
    "    index2word = dict(zip(word2index.values(), word2index.keys()))\n",
    "    \n",
    "    return data, word2index, index2word\n",
    "\n",
    "\n",
    "vocab_size = 50000\n",
    "data, word2index, index2word = build_dataset(words, vocab_size)\n",
    "print(len(data))\n",
    "print(data[:10])\n",
    "print([index2word[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "[['as', 'term', 'originated', 'of'], ['a', 'of', 'as', 'abuse'], ['term', 'abuse', 'a', 'first'], ['of', 'first', 'term', 'used'], ['abuse', 'used', 'of', 'against'], ['first', 'against', 'abuse', 'early'], ['used', 'early', 'first', 'working'], ['against', 'working', 'used', 'class'], ['early', 'class', 'against', 'radicals'], ['working', 'radicals', 'early', 'including']]\n",
      "['a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class']\n",
      "[13, 196, 3082, 3] 7\n",
      "44613.0\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "def generate_batch(data, begin_index, batch_size, bag_size=2):\n",
    "    assert begin_index >= bag_size\n",
    "    end_index = min(len(data)-2, begin_index+batch_size)\n",
    "    train_batch, train_labels = list(), list()\n",
    "    \n",
    "    for i in range(begin_index, end_index):\n",
    "        context = list()\n",
    "        target = data[i]\n",
    "        for j in range(1, bag_size+1):\n",
    "            context.append(data[i-j])\n",
    "            context.append(data[i+j])\n",
    "        train_batch.append(context)\n",
    "        train_labels.append(target)\n",
    "    if len(train_batch) < batch_size:\n",
    "        end_index, attach_batch, attach_labels = generate_batch(data, bag_size, \n",
    "                                        batch_size-len(train_batch), bag_size)\n",
    "        train_batch.extend(attach_batch)\n",
    "        if len(train_batch) < batch_size:\n",
    "            print('error')\n",
    "            print(len(train_batch))\n",
    "            print(attach_batch)\n",
    "        train_labels.extend(attach_labels)\n",
    "    return end_index, train_batch, train_labels\n",
    "\n",
    "begin_index = 3\n",
    "batch_size = 1000\n",
    "begin_index, train_batch, train_labels = generate_batch(data, begin_index, batch_size, bag_size=2)\n",
    "print(begin_index)\n",
    "print([index2word[i] for i in data[:10]])\n",
    "print([[index2word[i] for i in bi] for bi in train_batch[:10]])\n",
    "print([index2word[t] for t in train_labels[:10]])\n",
    "print(train_batch[0], train_labels[0])\n",
    "print(torch.max(torch.Tensor(train_labels)))\n",
    "print(len(train_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW (\n",
      "  (embedding): Embedding(50000, 128)\n",
      "  (linear1): Linear (512 -> 128)\n",
      "  (linear2): Linear (128 -> 50000)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, bag_size, batch_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.bag_size = bag_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.linear1 = nn.Linear(2*self.bag_size*self.embedding_size, self.embedding_size)\n",
    "        self.linear2 = nn.Linear(self.embedding_size, self.vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeded = self.embedding(inputs)\n",
    "#         print('embed: ', embeded.size())\n",
    "        embeded = embeded.view(1, -1)\n",
    "#         print('embeded size: ', embeded.size())\n",
    "        out = F.relu(self.linear1(embeded))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs\n",
    "\n",
    "    \n",
    "embedding_size = 128\n",
    "bag_size = 2\n",
    "batch_size = 8\n",
    "\n",
    "cbow = CBOW(vocab_size=vocab_size, embedding_size=embedding_size, \n",
    "            bag_size=bag_size, batch_size=batch_size)\n",
    "print(cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "   13\n",
      "  196\n",
      " 3082\n",
      "    3\n",
      "[torch.LongTensor of size 4]\n",
      "\n",
      "Variable containing:\n",
      " 7\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "torch.Size([1, 50000])\n",
      "\n",
      " 6463\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "x = autograd.Variable(torch.LongTensor(train_batch[0]))\n",
    "print(x)\n",
    "true_y = autograd.Variable(torch.LongTensor([train_labels[0]]))\n",
    "print(true_y)\n",
    "y = cbow(x)\n",
    "print(y.size())\n",
    "_, predict_y = torch.max(y.data, 1)\n",
    "print(predict_y)\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50000])\n",
      "\n",
      " 6463\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients\n",
      "error： 196\n",
      "torch.Size([1, 50000])\n",
      "\n",
      " 6463\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients\n",
      "error： 3\n",
      "torch.Size([1, 50000])\n",
      "\n",
      " 6463\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients\n",
      "error： 3135\n",
      "torch.Size([1, 50000])\n",
      "\n",
      " 6463\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients\n",
      "error： 47\n",
      "torch.Size([1, 50000])\n",
      "\n",
      " 6463\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients\n",
      "error： 60\n",
      "torch.Size([1, 50000])\n",
      "\n",
      " 6463\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients\n",
      "error： 157\n",
      "torch.Size([1, 50000])\n",
      "\n",
      " 6463\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients\n",
      "error： 129\n",
      "torch.Size([1, 50000])\n",
      "\n",
      " 6463\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients\n",
      "error： 196\n",
      "torch.Size([1, 50000])\n",
      "\n",
      " 6463\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients\n",
      "error： 3\n",
      "torch.Size([1, 50000])\n",
      "\n",
      " 6463\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients\n",
      "error： 3135\n",
      "torch.Size([1, 50000])\n",
      "\n",
      " 6463\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients\n",
      "error： 47\n",
      "torch.Size([1, 50000])\n",
      "\n",
      " 6463\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients\n",
      "error： 60\n",
      "torch.Size([1, 50000])\n",
      "\n",
      " 6463\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients\n",
      "error： 157\n",
      "torch.Size([1, 50000])\n",
      "\n",
      " 6463\n",
      "[torch.LongTensor of size 1x1]\n",
      "\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "torch.Size([1, 128])\n",
      "nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients\n",
      "error： 129\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(cbow.parameters(), lr=0.001)\n",
    "\n",
    "num_steps = 2\n",
    "begin_index = 3\n",
    "for i in range(num_steps):\n",
    "#     begin_index, train_batch, train_labels = generate_batch(data, begin_index, batch_size, bag_size=2)\n",
    "    c = 0\n",
    "    total_loss = 0\n",
    "    for j in range(1,batch_size):        \n",
    "        inputs = autograd.Variable(torch.LongTensor(train_batch[j]))\n",
    "    #     print('input size: ', inputs.size())\n",
    "        cbow.zero_grad()\n",
    "        out = cbow(inputs)\n",
    "        print(out.size())\n",
    "        _, predict_y = torch.max(y.data, 1)\n",
    "        print(predict_y)\n",
    "        try:\n",
    "            true_y = cbow.embedding(autograd.Variable(torch.LongTensor([train_labels[j]])))\n",
    "            predict_y = cbow.embedding(autograd.Variable(predict_y))\n",
    "            predict_y = predict_y.view(-1, 128)\n",
    "            print(type(true_y))\n",
    "            print(true_y.size())\n",
    "            print(type(predict_y))\n",
    "            print(predict_y.size())\n",
    "            loss = loss_function(predict_y, true_y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            c += 1\n",
    "            total_loss += loss.data\n",
    "            if c % 1000 == 0:\n",
    "                print('total loss', total_loss)\n",
    "                total_loss = 0\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('error：', train_labels[j])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_points = 200\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact')\n",
    "two_d_embeddings = tsne.fit_transform(cbow.embedding([1:num_points+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(embeddings, labels):\n",
    "  assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  pylab.figure(figsize=(15,15))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = embeddings[i,:]\n",
    "    pylab.scatter(x, y)\n",
    "    pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                   ha='right', va='bottom')\n",
    "  pylab.show()\n",
    "\n",
    "words = [index2word[i] for i in range(1, num_points+1)]\n",
    "plot(two_d_embeddings, words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
